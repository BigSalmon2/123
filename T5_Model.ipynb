{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T5_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMaE3NJHlsRAED5juOv4Y6C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "80e5f52cd9244b8d8944fad27aea36b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3b3aaac184ea4f0c99894a97ab207ea8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ad42790f70ea4723acc6e34dd341ae08",
              "IPY_MODEL_6a20b2f13f7a497ba565d59bba6d8b3b"
            ]
          }
        },
        "3b3aaac184ea4f0c99894a97ab207ea8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ad42790f70ea4723acc6e34dd341ae08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5cc6760197314f229342a7fe91f13b21",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 791656,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 791656,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_48ba72cf1863492993ff92d5198ff6ce"
          }
        },
        "6a20b2f13f7a497ba565d59bba6d8b3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0ff2800397454bd5b4a9cd6b85ba30f2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 792k/792k [00:02&lt;00:00, 389kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8b06a6e3631c49fca289d953d39af88d"
          }
        },
        "5cc6760197314f229342a7fe91f13b21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "48ba72cf1863492993ff92d5198ff6ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0ff2800397454bd5b4a9cd6b85ba30f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8b06a6e3631c49fca289d953d39af88d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dentadelta/123/blob/master/T5_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Pt9heQqcUM4",
        "colab_type": "text"
      },
      "source": [
        "# I borrows  a few lines of codes from the below Google Colabs:\n",
        "\n",
        "https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=coOmS2s_xDBy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtRtgREUcWd-",
        "colab_type": "code",
        "outputId": "cba2a567-21b4-475e-d3b3-30b3065c6ea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869
        }
      },
      "source": [
        "!pip install nlp transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/69/17c95e9bdb431bb5102f331d3d34e0f3aabef14a8041690ad72c2b11d1d0/nlp-0.2.0-py3-none-any.whl (857kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 860kB 2.8MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 675kB 16.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from nlp) (2.23.0)\n",
            "Collecting pyarrow>=0.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/3f/6cac1714fff444664603f92cb9fbe91c7ae25375880158b9e9691c4584c8/pyarrow-0.17.1-cp36-cp36m-manylinux2014_x86_64.whl (63.8MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63.8MB 48kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nlp) (1.18.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from nlp) (4.41.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from nlp) (0.3.1.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from nlp) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from nlp) (3.0.12)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 41.7MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 33.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.8MB 35.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=a30044918b5b7b9bd5c6db92b9a286591653dfdcbbec2df4fd934a3e15a851d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: pyarrow, nlp, sentencepiece, sacremoses, tokenizers, transformers\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed nlp-0.2.0 pyarrow-0.17.1 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyarrow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKGyupu2cvgC",
        "colab_type": "text"
      },
      "source": [
        "Make sure you reload the runtime after you run the above codes\n",
        "\n",
        "Also make sure you change your runtime to GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHOjk_wP5BS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "from transformers import (T5Config, T5Tokenizer, T5ForConditionalGeneration, TextDataset, DataCollator, Trainer, TrainingArguments)\n",
        "import ipywidgets as widgets\n",
        "import random\n",
        "from typing import Dict, List, Optional, Union\n",
        "import nlp\n",
        "from dataclasses import dataclass\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adm8d-N0GNLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyarrow as pa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcwzoJ-iEbvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained_model_name = 't5-base'   # options are: t5-small, t5-base, t5-large, t5-3B, t5-11B\n",
        "training_file_path = ''   # I downloaded the csv from dropbox using the below url\n",
        "working_folder = '/content/'\n",
        "\n",
        "os.environ['First_Time_Training_The_Model'] = 'True'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pU8Gf-h97cl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url =  'https://www.dropbox.com/s/6w5z4qvt8vytngm/training_data.csv?dl=1' # If you save your data in dropbox then anyone who has the link can access to the data in real time :P\n",
        "df=pd.read_csv(url)\n",
        "df = df[:10000]   # Only load the first 10000 dataset (You can train the entire dataset if you like)\n",
        "\n",
        "# df['prefix'] = 'seq2seq'  Uncomment this line if you are not training a question to answer model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnCj9LNABVrg",
        "colab_type": "code",
        "outputId": "7174d2b1-387c-434b-ec90-66f38c741ea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prefix</th>\n",
              "      <th>input_text</th>\n",
              "      <th>target_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
              "      <td>Saint Bernadette Soubirous</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>What is in front of the Notre Dame Main Building?</td>\n",
              "      <td>a copper statue of Christ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
              "      <td>the Main Building</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>What is the Grotto at Notre Dame?</td>\n",
              "      <td>a Marian place of prayer and reflection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>What sits on top of the Main Building at Notre...</td>\n",
              "      <td>a golden statue of the Virgin Mary</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>Institutes of technology with different origin...</td>\n",
              "      <td>What is the name of Thammasat University's eng...</td>\n",
              "      <td>Sirindhorn International Institute of Technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>Institutes of technology with different origin...</td>\n",
              "      <td>What is Thailand's only government-established...</td>\n",
              "      <td>Suranaree University of Technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>Institutes of technology with different origin...</td>\n",
              "      <td>What year was Suranaree University of Technolo...</td>\n",
              "      <td>1989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>Institutes of technology with different origin...</td>\n",
              "      <td>What is the name of the best-known private ins...</td>\n",
              "      <td>Mahanakorn University of Technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>In Turkey and the Ottoman Empire, the oldest t...</td>\n",
              "      <td>What institute of technology opened in Bursa i...</td>\n",
              "      <td>Bursa Technical University</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 prefix  ...                                       target_text\n",
              "0     Architecturally, the school has a Catholic cha...  ...                        Saint Bernadette Soubirous\n",
              "1     Architecturally, the school has a Catholic cha...  ...                         a copper statue of Christ\n",
              "2     Architecturally, the school has a Catholic cha...  ...                                 the Main Building\n",
              "3     Architecturally, the school has a Catholic cha...  ...           a Marian place of prayer and reflection\n",
              "4     Architecturally, the school has a Catholic cha...  ...                a golden statue of the Virgin Mary\n",
              "...                                                 ...  ...                                               ...\n",
              "9995  Institutes of technology with different origin...  ...  Sirindhorn International Institute of Technology\n",
              "9996  Institutes of technology with different origin...  ...                Suranaree University of Technology\n",
              "9997  Institutes of technology with different origin...  ...                                              1989\n",
              "9998  Institutes of technology with different origin...  ...               Mahanakorn University of Technology\n",
              "9999  In Turkey and the Ottoman Empire, the oldest t...  ...                        Bursa Technical University\n",
              "\n",
              "[10000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfmywdQVgZCV",
        "colab_type": "code",
        "outputId": "1f211315-d14f-4d75-f5ac-80ed9a304ca1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "df['prefix'][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fOjRTWUgduH",
        "colab_type": "code",
        "outputId": "a4c05cc4-d37a-4aa3-c47a-484e41f703b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "length_of_prefix = len(df['prefix'][0])\n",
        "length_of_prefix  # Prefix is the context document"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "695"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQbgfVMtcO8o",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImbMyhfEEMzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_eos_to_examples(example):\n",
        "    example['input_text'] = '<{}>: <{}> </s>'.format(example['prefix'] , example['input_text'] )\n",
        "    example['target_text'] = '\"<{}> </s>\"'.format(example['target_text'])\n",
        "    return example\n",
        "\n",
        "def convert_to_features(example_batch):\n",
        "    input_encodings = tokenizer.batch_encode_plus(example_batch['input_text'], pad_to_max_length=True, max_length=1700)     ########## Specify the maximum input lengths (context + question)\n",
        "    target_encodings = tokenizer.batch_encode_plus(example_batch['target_text'], pad_to_max_length=True, max_length=16)     ########## Specify the maximum output length\n",
        "    encodings = {\n",
        "        'input_ids': input_encodings['input_ids'], \n",
        "        'attention_mask': input_encodings['attention_mask'],\n",
        "        'target_ids': target_encodings['input_ids'],\n",
        "        'target_attention_mask': target_encodings['attention_mask']\n",
        "    }\n",
        "    return encodings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9GzwnjYEUmf",
        "colab_type": "code",
        "outputId": "569c5d36-e0e1-43ee-f849-d6b2162a3d7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "80e5f52cd9244b8d8944fad27aea36b0",
            "3b3aaac184ea4f0c99894a97ab207ea8",
            "ad42790f70ea4723acc6e34dd341ae08",
            "6a20b2f13f7a497ba565d59bba6d8b3b",
            "5cc6760197314f229342a7fe91f13b21",
            "48ba72cf1863492993ff92d5198ff6ce",
            "0ff2800397454bd5b4a9cd6b85ba30f2",
            "8b06a6e3631c49fca289d953d39af88d"
          ]
        }
      },
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(pretrained_model_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80e5f52cd9244b8d8944fad27aea36b0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=791656.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8Tvj52oDK9H",
        "colab_type": "code",
        "outputId": "869a0686-93cf-46e9-b978-6b04e51f493f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "fields = [\n",
        "          ('input_text', pa.string()),\n",
        "          ('target_text', pa.string()),\n",
        "          ('prefix', pa.string())\n",
        "      ]\n",
        "\n",
        "train_dataset = nlp.arrow_dataset.Dataset(pa.Table.from_pandas(df,pa.schema(fields)))\n",
        "train_dataset = train_dataset.map(add_eos_to_examples)\n",
        "train_dataset = train_dataset.map(convert_to_features, batched=True)\n",
        "columns = ['input_ids', 'target_ids', 'attention_mask', 'target_attention_mask']\n",
        "train_dataset.set_format(type='torch', columns=columns)\n",
        "torch.save(train_dataset, '{}/train_data.pt'.format(working_folder))\n",
        "\n",
        "# Make sure you restart the runtime, then rerun from the second cell or you will get an error"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000it [00:00, 26181.66it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:10<00:00,  1.08s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NvRSKlBDx8g",
        "colab_type": "code",
        "outputId": "c37b9251-6cce-4741-b305-45cff62eb223",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "tokenizer.save_pretrained(working_folder)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/spiece.model',\n",
              " '/content/special_tokens_map.json',\n",
              " '/content/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG0rFa8qHMU0",
        "colab_type": "code",
        "outputId": "be033dd3-a55f-44b1-9b86-82dc2253b03f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "number_of_rows = len(df)\n",
        "number_of_rows"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyU_LQySHQPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@dataclass\n",
        "class T2TDataCollator(DataCollator):\n",
        "    def collate_batch(self, batch: List) -> Dict[str, torch.Tensor]:\n",
        "        input_ids = torch.stack([example['input_ids'] for example in batch])\n",
        "        lm_labels = torch.stack([example['target_ids'] for example in batch])\n",
        "        lm_labels[lm_labels[:, :] == 0] = -100\n",
        "        attention_mask = torch.stack([example['attention_mask'] for example in batch])\n",
        "        decoder_attention_mask = torch.stack([example['target_attention_mask'] for example in batch])\n",
        "        \n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids, \n",
        "            'attention_mask': attention_mask,\n",
        "            'lm_labels': lm_labels, \n",
        "            'decoder_attention_mask': decoder_attention_mask\n",
        "        }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIAuBbzRHTux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(first_time= os.environ['First_Time_Training_The_Model'] , working_folder=working_folder, pretrained_model_name=pretrained_model_name):\n",
        "  if first_time == 'True':\n",
        "    config = T5Config.from_pretrained(pretrained_model_name)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(pretrained_model_name, config=config)\n",
        "    print('Creating a brand new T5 model')\n",
        "\n",
        "  else:\n",
        "    model = T5ForConditionalGeneration.from_pretrained(working_folder)\n",
        "    print('Loading the T5 model that has been trained previously')\n",
        "  train_dataset = torch.load('{}/train_data.pt'.format(working_folder))\n",
        "  data_collator = T2TDataCollator()\n",
        "\n",
        "  return model, train_dataset, data_collator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQwZcHYjYMLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(working_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qww8XEQCV-Gl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change the os.environment to True if you mess up with loading the model\n",
        "#os.environ['First_Time_Training_The_Model'] = 'True'  ############ Only run if needs to"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UnpLOH6IZsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 1 ### Specifiy how many times you want the machine to loop over your data\n",
        "\n",
        "model,train_dataset, data_collator = load_model()   #### Change to false after you run it the first time\n",
        "progress = widgets.FloatProgress(value=0.1, min=0.0, max=1.0, bar_style = 'info')\n",
        "training_args = TrainingArguments(\n",
        "                    output_dir= working_folder,\n",
        "                    overwrite_output_dir=True,\n",
        "                    do_train=True,\n",
        "                    num_train_epochs=epochs,   \n",
        "                    per_device_train_batch_size=2, ### Specify how many chunks the data should be splitted\n",
        "                    logging_steps=1000,    ### Specify how often you want to see the loss outputs\n",
        "                    save_steps=-1)\n",
        "trainer = Trainer(\n",
        "                    model=model,\n",
        "                    args=training_args,\n",
        "                    data_collator=data_collator,\n",
        "                    train_dataset=train_dataset,\n",
        "                    prediction_loss_only=True)\n",
        "  \n",
        "\n",
        "progress.value = 0.4\n",
        "p_start, p_end = 0.4, 1.\n",
        "  \n",
        "def progressify(f):\n",
        "  def inner(*args, **kwargs):\n",
        "    if trainer.epoch is not None:\n",
        "      progress.value = p_start + trainer.epoch / epochs * (p_end - p_start)\n",
        "      return f(*args, **kwargs)\n",
        "  return inner\n",
        "trainer._training_step = progressify(trainer._training_step)\n",
        "trainer.train()\n",
        "trainer.save_model(working_folder)\n",
        "os.environ['First_Time_Training_The_Model'] = 'False'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5mfN_juJETo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In case you loose patient, you can still save the model to keeping later\n",
        "trainer.save_model(working_folder)\n",
        "os.environ['First_Time_Training_The_Model'] = 'False'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSXdSYtCM5G-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_dataset = torch.load('{}/train_data.pt'.format(working_folder))    #Dont have valid data_set...\n",
        "dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTv6Q0pGNDKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluate_model = T5ForConditionalGeneration.from_pretrained(working_folder).to('cuda')  # Make sure you set your runtime to GPU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtKCe1QUNI6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answers = []\n",
        "for batch in tqdm(dataloader):\n",
        "  outs = evaluate_model.generate(input_ids=batch['input_ids'].to('cuda'), \n",
        "                        attention_mask=batch['attention_mask'].to('cuda'),\n",
        "                        max_length=1500,\n",
        "                        early_stopping=True)\n",
        "  outs = [tokenizer.decode(ids) for ids in outs]\n",
        "  answers.extend(outs)\n",
        "\n",
        "  # You can interupt the execution. Data loaded to 'answers' array wont get lost (but make sure you have at least 5 data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELX68l-SNLkl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = []\n",
        "references = []\n",
        "input_texts = []\n",
        "for ref, pred in zip(valid_dataset, answers):\n",
        "  pred = pred[4:-1]\n",
        "  predictions.append(pred)\n",
        "  \n",
        "  input_ = tokenizer.decode(ref['input_ids'])\n",
        "  input_ = ''.join(input_)[0:-3]\n",
        "  input_ = re.sub('[!@#$*-]', '', input_)\n",
        "  input_ = input_.lstrip().title()\n",
        "\n",
        "  start_index = input_.index('>:')\n",
        "  prefix = input_[:start_index]\n",
        "  input_ = input_[start_index + 6:]\n",
        "\n",
        "  input_texts.append(input_)\n",
        "\n",
        "  output_ = tokenizer.decode(ref['target_ids'])\n",
        "  output_ = ''.join(output_)[4:-3]\n",
        "  references.append(output_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLKrcCtlNOVR",
        "colab_type": "code",
        "outputId": "f89c8b14-61b2-4162-bbb6-ce571a0177f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "for _ in range(min(5, len(answers))):\n",
        "  i = random.randint(0, len(predictions))\n",
        "  print('Input:             {}\\nPredicted Answer:  {}\\nReal Answer:       {}\\n'.format(input_texts[i],predictions[i], references[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:             How Often Is Catholic Mass Held At Notre Dame In A Wee\n",
            "Predicted Answer:  Catholic Mass\n",
            "Real Answer:       over 100 times\n",
            "\n",
            "Input:             There Are How Many Dorms For Females At Notre Dam\n",
            "Predicted Answer:  there are how many dorms for females at Notre Dame\n",
            "Real Answer:       14\n",
            "\n",
            "Input:             On What Date Was The Rebuilding Of The Main Building Begun At Notre Dame After The Fire That Claimed The Previou\n",
            "Predicted Answer:  the library collection\n",
            "Real Answer:       17th of May\n",
            "\n",
            "Input:             In What Building Is The Current School Of Architecture Housed At Notre Dam\n",
            "Predicted Answer:  School of Architecture\n",
            "Real Answer:       Bond Hall\n",
            "\n",
            "Input:             What Is The Annual Budget Of Notre Dame'S Lafortune Cente\n",
            "Predicted Answer:  LaFortune\n",
            "Real Answer:       $1.2 million\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXKtTm0-N6jU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If the question is in completed, it is because I dont have enough memmory to train the full context (maximum input length is only 1700 words). Refer to cell 4\n",
        "# I only trained the model for 44 seconds, so this is why the answer isnt accurate...\n",
        "# Excuse my english or grammar, I got an idea and my idea worth a million words"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}